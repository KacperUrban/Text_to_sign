{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Load pretrained model locally"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-06 14:12:05.699982: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-06 14:12:05.996197: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-06 14:12:05.997698: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 14:12:07.335806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at ./model/model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/tokenizer\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"./model/model.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T14:12:24.225785239Z",
     "start_time": "2023-05-06T14:12:03.167977316Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_marian_mt_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (TFMarianMainLayer)   multiple                  77138944  \n",
      "                                                                 \n",
      " final_logits_bias (BiasLaye  multiple                 63430     \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77,202,374\n",
      "Trainable params: 77,138,944\n",
      "Non-trainable params: 63,430\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T14:12:24.295231978Z",
     "start_time": "2023-05-06T14:12:24.219124836Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'model',\n 'trainable': True,\n 'dtype': 'float32',\n 'config': {'vocab_size': 63430,\n  'decoder_vocab_size': 63430,\n  'max_position_embeddings': 512,\n  'd_model': 512,\n  'encoder_ffn_dim': 2048,\n  'encoder_layers': 6,\n  'encoder_attention_heads': 8,\n  'decoder_ffn_dim': 2048,\n  'decoder_layers': 6,\n  'decoder_attention_heads': 8,\n  'dropout': 0.1,\n  'attention_dropout': 0.0,\n  'activation_dropout': 0.0,\n  'activation_function': 'swish',\n  'init_std': 0.02,\n  'encoder_layerdrop': 0.0,\n  'decoder_layerdrop': 0.0,\n  'use_cache': True,\n  'num_hidden_layers': 6,\n  'scale_embedding': True,\n  'share_encoder_decoder_embeddings': True,\n  'return_dict': True,\n  'output_hidden_states': False,\n  'output_attentions': False,\n  'torchscript': False,\n  'torch_dtype': None,\n  'use_bfloat16': False,\n  'tf_legacy_loss': False,\n  'pruned_heads': {},\n  'tie_word_embeddings': True,\n  'is_encoder_decoder': True,\n  'is_decoder': False,\n  'cross_attention_hidden_size': None,\n  'add_cross_attention': False,\n  'tie_encoder_decoder': False,\n  'max_length': 512,\n  'min_length': 0,\n  'do_sample': False,\n  'early_stopping': False,\n  'num_beams': 6,\n  'num_beam_groups': 1,\n  'diversity_penalty': 0.0,\n  'temperature': 1.0,\n  'top_k': 50,\n  'top_p': 1.0,\n  'typical_p': 1.0,\n  'repetition_penalty': 1.0,\n  'length_penalty': 1.0,\n  'no_repeat_ngram_size': 0,\n  'encoder_no_repeat_ngram_size': 0,\n  'bad_words_ids': [[63429]],\n  'num_return_sequences': 1,\n  'chunk_size_feed_forward': 0,\n  'output_scores': False,\n  'return_dict_in_generate': False,\n  'forced_bos_token_id': None,\n  'forced_eos_token_id': 0,\n  'remove_invalid_values': False,\n  'exponential_decay_length_penalty': None,\n  'suppress_tokens': None,\n  'begin_suppress_tokens': None,\n  'architectures': ['MarianMTModel'],\n  'finetuning_task': None,\n  'id2label': {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'},\n  'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2},\n  'tokenizer_class': None,\n  'prefix': None,\n  'bos_token_id': 0,\n  'pad_token_id': 63429,\n  'eos_token_id': 0,\n  'sep_token_id': None,\n  'decoder_start_token_id': 63429,\n  'task_specific_params': None,\n  'problem_type': None,\n  '_name_or_path': './model/model.h5',\n  'transformers_version': '4.27.4',\n  '_num_labels': 3,\n  'add_bias_logits': False,\n  'add_final_layer_norm': False,\n  'classif_dropout': 0.0,\n  'classifier_dropout': 0.0,\n  'model_type': 'marian',\n  'normalize_before': False,\n  'normalize_embedding': False,\n  'static_position_embeddings': True}}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.get_config()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T14:12:44.049205512Z",
     "start_time": "2023-05-06T14:12:44.025350814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'encoder',\n 'trainable': True,\n 'dtype': 'float32',\n 'config': {'vocab_size': 63430,\n  'decoder_vocab_size': 63430,\n  'max_position_embeddings': 512,\n  'd_model': 512,\n  'encoder_ffn_dim': 2048,\n  'encoder_layers': 6,\n  'encoder_attention_heads': 8,\n  'decoder_ffn_dim': 2048,\n  'decoder_layers': 6,\n  'decoder_attention_heads': 8,\n  'dropout': 0.1,\n  'attention_dropout': 0.0,\n  'activation_dropout': 0.0,\n  'activation_function': 'swish',\n  'init_std': 0.02,\n  'encoder_layerdrop': 0.0,\n  'decoder_layerdrop': 0.0,\n  'use_cache': True,\n  'num_hidden_layers': 6,\n  'scale_embedding': True,\n  'share_encoder_decoder_embeddings': True,\n  'return_dict': True,\n  'output_hidden_states': False,\n  'output_attentions': False,\n  'torchscript': False,\n  'torch_dtype': None,\n  'use_bfloat16': False,\n  'tf_legacy_loss': False,\n  'pruned_heads': {},\n  'tie_word_embeddings': True,\n  'is_encoder_decoder': True,\n  'is_decoder': False,\n  'cross_attention_hidden_size': None,\n  'add_cross_attention': False,\n  'tie_encoder_decoder': False,\n  'max_length': 512,\n  'min_length': 0,\n  'do_sample': False,\n  'early_stopping': False,\n  'num_beams': 6,\n  'num_beam_groups': 1,\n  'diversity_penalty': 0.0,\n  'temperature': 1.0,\n  'top_k': 50,\n  'top_p': 1.0,\n  'typical_p': 1.0,\n  'repetition_penalty': 1.0,\n  'length_penalty': 1.0,\n  'no_repeat_ngram_size': 0,\n  'encoder_no_repeat_ngram_size': 0,\n  'bad_words_ids': [[63429]],\n  'num_return_sequences': 1,\n  'chunk_size_feed_forward': 0,\n  'output_scores': False,\n  'return_dict_in_generate': False,\n  'forced_bos_token_id': None,\n  'forced_eos_token_id': 0,\n  'remove_invalid_values': False,\n  'exponential_decay_length_penalty': None,\n  'suppress_tokens': None,\n  'begin_suppress_tokens': None,\n  'architectures': ['MarianMTModel'],\n  'finetuning_task': None,\n  'id2label': {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'},\n  'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2},\n  'tokenizer_class': None,\n  'prefix': None,\n  'bos_token_id': 0,\n  'pad_token_id': 63429,\n  'eos_token_id': 0,\n  'sep_token_id': None,\n  'decoder_start_token_id': 63429,\n  'task_specific_params': None,\n  'problem_type': None,\n  '_name_or_path': './model/model.h5',\n  'transformers_version': '4.27.4',\n  '_num_labels': 3,\n  'add_bias_logits': False,\n  'add_final_layer_norm': False,\n  'classif_dropout': 0.0,\n  'classifier_dropout': 0.0,\n  'model_type': 'marian',\n  'normalize_before': False,\n  'normalize_embedding': False,\n  'static_position_embeddings': True}}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encoder.get_config()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T14:13:03.267921743Z",
     "start_time": "2023-05-06T14:13:03.215956086Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl - en\n",
      "Max lenght: 512\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.source_lang, \"-\", tokenizer.target_lang)\n",
    "print(f\"Max lenght: {tokenizer.model_max_length}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T14:13:22.864155242Z",
     "start_time": "2023-05-06T14:13:22.810799287Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The attempt of implementation a transfer learning on a model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data from file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Wczytanie plików z danymi\n",
    "with open(\"dataset/polish.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    polish_data = f.read().splitlines()\n",
    "\n",
    "with open(\"dataset/english.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    english_data = f.read().splitlines()\n",
    "\n",
    "raw_dataset_list = []\n",
    "for i in range(0, len(polish_data)):\n",
    "    raw_dataset_list.append({'translation' : {'pl' : polish_data[i], 'en' : english_data[i]}})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T15:08:50.555823620Z",
     "start_time": "2023-05-06T15:08:48.095432491Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'translation': {'pl': 'Polska', 'en': 'Poland'}},\n {'translation': {'pl': 'Europejski Fundusz Społeczny w Polsce, 2007-2013',\n   'en': 'The European Social Fund in Poland, 2007-2013'}},\n {'translation': {'pl': 'Finansowany z EFS Program Operacyjny Kapitał Ludzki 2007-2013 świadczy o zaangażowaniu się Polski w inwestowanie w ludzi i ich umiejętności oraz o gotowości do podejmowania wyzwań, przed którymi obecnie staje.',\n   'en': 'Poland’s ESF Operational Programme for 2007-2013 – Human Capital – is evidence of the country’s commitment to investing in people and their skills and determination to tackle the challenges Poland faces.'}},\n {'translation': {'pl': 'Położenie większego nacisku na edukację i szkolenia wydaje się być najlepszym sposobem na zwalczanie bezrobocia, wykluczenia społecznego oraz wyrównywania różnic między regionami.',\n   'en': 'More education and training is the key instrument for combating unemployment, social exclusion and strong regional disparities.'}},\n {'translation': {'pl': 'Udoskonalenie systemów edukacyjnych i szkoleniowych, reformy rynku pracy oraz wzmocnienie sektora ekonomii społecznej pozwolą osiągnąć długotrwały sukces.',\n   'en': 'Improvements to the education and training systems, labour market and the social-economy sector will strengthen capacity to deliver sustainable success.'}},\n {'translation': {'pl': 'Europejski Fundusz Społeczny w kilku słowach',\n   'en': 'The European Social Fund in brief'}},\n {'translation': {'pl': 'Europejski Fundusz Społeczny, utworzony w 1957 roku, jest głównym narzędziem nansowym Unii Europejskiej służącym do inwestowania w kapitał ludzki.',\n   'en': 'The European Social Fund, created in 1957, is the European Union’s main financial instrument for investing in people.'}},\n {'translation': {'pl': 'Wspiera zatrudnienie i pomaga ludziom zdobywać lepsze wykształcenie i umiejętności, co z kolei zwiększa ich szanse na rynku pracy.',\n   'en': 'It supports employment and helps people enhance their education and skills. This improves their job prospects.'}},\n {'translation': {'pl': 'Państwa członkowskie i regiony opracowują własne Programy Operacyjne EFS, które stanowią reakcję na realne potrzeby zaistniałe„w terenie”.',\n   'en': 'Member States and regions devise their own ESF Operational Programmes, in order to respond to the real needs‘on the ground’.'}},\n {'translation': {'pl': 'W okresie 2007-2013, we wszystkich krajach unijnych, ze środków EFS wydane zostanie ponad 10 miliardów euro rocznie.',\n   'en': 'Over the period 2007-2013, the ESF will spend over 10 billion euro per year across all Member States.'}}]"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset_list[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T15:08:51.492183192Z",
     "start_time": "2023-05-06T15:08:51.487215985Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create preprocessing function for our data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "def preprocess_fun(inputs, targets):\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding='max_length')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding='max_length')\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T12:51:22.445973012Z",
     "start_time": "2023-05-06T12:51:22.437446117Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3586: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "idx = round(len(polish_data)*0.95)\n",
    "train_data = preprocess_fun(polish_data[:idx], english_data[:idx])\n",
    "val_data = preprocess_fun(polish_list[idx:], english_list[idx:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T12:51:23.067634308Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
