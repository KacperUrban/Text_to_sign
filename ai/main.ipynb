{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Load pretrained model locally"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/tokenizer\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"./model/model.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.model.get_config()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.model.encoder.get_config()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tokenizer.source_lang, \"-\", tokenizer.target_lang)\n",
    "print(f\"Max lenght: {tokenizer.model_max_length}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The attempt of implementation a transfer learning on a model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data from file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polish_list = []\n",
    "english_list = []\n",
    "with open(\"./dataset/polish.txt\", \"r+\") as file1:\n",
    "    for line in file1:\n",
    "        line = line.strip()\n",
    "        polish_list.append(line)\n",
    "\n",
    "with open(\"./dataset/english.txt\", \"r+\") as file1:\n",
    "    for line in file1:\n",
    "        line = line.strip()\n",
    "        english_list.append(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Polska', 'Europejski Fundusz Społeczny w Polsce, 2007-2013', 'Finansowany z EFS Program Operacyjny Kapitał Ludzki 2007-2013 świadczy o zaangażowaniu się Polski w inwestowanie w ludzi i ich umiejętności oraz o gotowości do podejmowania wyzwań, przed którymi obecnie staje.', 'Położenie większego nacisku na edukację i szkolenia wydaje się być najlepszym sposobem na zwalczanie bezrobocia, wykluczenia społecznego oraz wyrównywania różnic między regionami.', 'Udoskonalenie systemów edukacyjnych i szkoleniowych, reformy rynku pracy oraz wzmocnienie sektora ekonomii społecznej pozwolą osiągnąć długotrwały sukces.']\n",
      "['Poland', 'The European Social Fund in Poland, 2007-2013', 'Poland’s ESF Operational Programme for 2007-2013 – Human Capital – is evidence of the country’s commitment to investing in people and their skills and determination to tackle the challenges Poland faces.', 'More education and training is the key instrument for combating unemployment, social exclusion and strong regional disparities.', 'Improvements to the education and training systems, labour market and the social-economy sector will strengthen capacity to deliver sustainable success.']\n"
     ]
    }
   ],
   "source": [
    "print(polish_list[:5])\n",
    "print(english_list[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-29T17:35:21.591169659Z",
     "start_time": "2023-04-29T17:35:21.552383678Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Scalar tensor has no `len()`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43melement\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43melement\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtf_polish\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43melement\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m element \u001B[38;5;129;01min\u001B[39;00m tf_polish)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1107\u001B[0m, in \u001B[0;36m_EagerTensorBase.__len__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1105\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns the length of the first dimension in the Tensor.\"\"\"\u001B[39;00m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mndims:\n\u001B[0;32m-> 1107\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScalar tensor has no `len()`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   1109\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mTypeError\u001B[0m: Scalar tensor has no `len()`"
     ]
    }
   ],
   "source": [
    "max(len(element) for element in polish_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-29T17:37:34.137110135Z",
     "start_time": "2023-04-29T17:37:33.886314529Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create preprocessing function for our data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "def preprocess_fun(inputs, targets):\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding='max_length')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding='max_length')\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3586: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "idx = round(len(polish_list)*0.95)\n",
    "train_data = preprocess_fun(polish_list[:idx], english_list[:idx])\n",
    "val_data = preprocess_fun(polish_list[idx:], english_list[idx:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_ids:[3285, 0, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429], attention mask:[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label:[1480, 0, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429, 63429] dla slowa Polska\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input_ids:{train_data['input_ids'][0]}, attention mask:{train_data['attention_mask'][0]}, label:{train_data['labels'][0]} dla slowa {polish_list[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-29T17:07:29.252400975Z",
     "start_time": "2023-04-29T17:07:29.250019616Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
