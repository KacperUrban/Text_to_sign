{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Import necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e90ad30c09b9dc77"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from odf.opendocument import load\n",
    "from odf import text, teletype\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from tensorflow import keras\n",
    "import neptune\n",
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "from transformers import pipeline\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:07:08.263472032Z",
     "start_time": "2023-10-30T20:07:03.845864808Z"
    }
   },
   "id": "46b5512e5c528ccf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data from .odt format"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd35bdf3cf4e919"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:07:16.333099856Z",
     "start_time": "2023-10-30T20:07:16.324796941Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    raw_data = []\n",
    "    text_doc = load(filepath)\n",
    "    all_params = text_doc.getElementsByType(text.P)\n",
    "    for line in all_params:\n",
    "        raw_data.append(teletype.extractText(line))\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['Wypowiedzi OG',\n 'Przywitanie',\n '- Dzień dobry.',\n '[',\n 'dzień dobry',\n 'witać',\n ']',\n 'Wyjaśnienie powodu wizyty',\n '- Chcę złożyć wniosek o wydanie dowodu osobistego.',\n '[',\n 'ja wniosek dowód1 mieć',\n 'ja wniosek dowód2 mieć',\n ']',\n '- Czy mogę odebrać dowód?',\n '[',\n 'czy już nowy dowód1',\n 'czy już nowy dowód2',\n 'być mój nowy dowód1',\n 'być mój nowy dowód2',\n ']',\n '- Chcę zgłosić utratę dowodu osobistego.',\n '[',\n 'ja dowód1 zgubić',\n 'ja dowód2 zgubić',\n ']',\n '- Chcę złożyć wniosek o wydanie dowodu osobistego dla mojego dziecka.',\n '[',\n 'ja dowód1 moje dziecko chcieć',\n 'ja dowód2 moje dziecko chcieć',\n ']',\n '- Chcę złożyć wniosek o wydanie dowodu osobistego dla mojej żony.',\n '[',\n 'ja dowód1 moja żona chcieć',\n 'ja dowód2 moja żona chcieć',\n ']',\n 'Obsługa',\n '- Zmieniłem adres.',\n '[',\n 'dom przenieść się',\n 'mieszkanie zmiana1',\n 'mieszkanie zmiana2',\n 'adres1 zmiana1',\n 'adres2 zmiana1',\n 'adres1 zmiana2',\n 'adres2 zmiana2',\n ']',\n '- Upłynął termin ważności dowodu.',\n '[',\n 'dowód1 nieważny',\n 'dowód2 nieważny',\n 'dowód1 koniec ważny',\n 'dowód2 koniec ważny',\n ']',\n '- Z powodu zmiany wizerunku twarzy.',\n '[',\n 'z powód zmiana wygląd',\n ']',\n '- Zawarłem(am) związek małżeński.',\n '[',\n 'ja małżeństwo',\n ']',\n '- Zmieniłem(am) nazwisko.',\n '[',\n 'ja zmiana1 nazwisko',\n 'ja zmiana2 nazwisko',\n ']',\n '- Decyzją zmieniłam nazwisko.',\n '[',\n ']',\n '- Powróciłam do nazwiska rodowego.',\n '[',\n 'ja przyjmować nazwisko panna',\n 'ja przyjmować nazwisko kawaler',\n ']',\n '- Nastąpiła zmiana imienia ojca.',\n '[',\n 'ojcieć imię zmiana1',\n 'ojcieć imię zmiana2',\n ']',\n '- Nastąpiła zmiana imienia matki.',\n '[',\n 'matka imię zmiana1',\n 'matka imię zmiana2',\n ']',\n '- Tak.',\n '[',\n 'tak',\n ']',\n '- Dlaczego?',\n '[',\n 'dlaczego',\n ']',\n '- Nie.',\n '[',\n 'nie',\n ']',\n '- Dziękuję.',\n '[',\n 'dziękować1',\n 'dziękować2',\n ']',\n '- Nie umiem wypełnić wniosku.',\n '[',\n 'ja nie wiedzieć wypełnić',\n ']',\n '- Proszę.',\n '[',\n 'prosić',\n ']',\n '- Nie mam dowodu.',\n '[',\n 'niemieć dowód1',\n 'niemieć dowód2',\n ']',\n '- Nie zabrałem ze sobą dowodu.',\n '[',\n 'niemieć dowód1',\n 'niemieć dowód2',\n ']',\n '- To mój pierwszy dowód.',\n '[',\n 'ja pierwszy raz dowód1 robić',\n 'ja pierwszy raz dowód2 robić',\n ']',\n '- Nie mam zdjęcia.',\n '[',\n 'niemieć zdjęcie',\n 'zdjęcie niemieć',\n ']',\n '- Czy można zgłosić się wcześniej?',\n '[',\n 'dowód1 odbiór wcześniej można',\n 'dowód2 odbiór wcześniej można',\n ']',\n '- Czy można przyśpieszyć produkcję dowodu?',\n '[',\n 'dowód1 wcześniej gotowy można',\n 'dowód2 wcześniej gotowy można',\n ']',\n '- Ile płacę za wyrobienie dowodu?',\n '[',\n 'nowy dowód1 płacić ile',\n 'nowy dowód2 płacić ile',\n ']',\n '- Nie umiem wypełnić formularza.',\n '[',\n 'formularz wypełnić nieumieć',\n ']',\n '- Mam uszkodzony dowód.',\n '[',\n 'mój dowód1 pękąć',\n 'mój dowód2 pękąć',\n 'mój dowód1 psuć',\n 'mój dowód2 psuć',\n 'mój dowód1 zniszczyć',\n 'mój dowód2 zniszczyć',\n ']',\n 'Pożegnanie',\n '- Do widzenia.',\n '[',\n 'do widzenia',\n ']',\n 'Dodatkowe',\n '- Chcę złożyć wniosek o wydanie dowodu osobistego.',\n '[',\n 'chcieć nowy dowód1',\n 'chcieć nowy dowód2',\n 'nowy dowód1 chcieć',\n 'nowy dowód2 chcieć',\n ']',\n '- Czy mogę odebrać dowód?',\n '[',\n 'dowód1 gotowy',\n 'dowód2 gotowy',\n ']',\n '- Nie umiem wypełnić wniosku.',\n '[',\n 'papier wniosek wypełnić jak',\n ']',\n '- Nie umiem wypełnić wniosku.',\n '[',\n 'ja nie rozumieć co to być',\n 'ja nie rozumieć co pisac',\n ']',\n '- Nie umiem wypełnić wniosku.',\n '[',\n 'nie rozumieć to',\n 'niewiedzieć to',\n 'nie rozumieć - pomysł',\n ']',\n '- Nie umiem wypełnić wniosku.',\n '[',\n 'co pisać mieć',\n 'jak pisać mieć',\n ']',\n '- Proszę mi pomóc.',\n '[',\n 'prosić pomagać ja',\n 'ja niewiedzieć prosić pomagać ja pusto',\n ']',\n '- Ile płacę za wyrobienie dowodu?',\n '[',\n 'pieniądze ile nowy dowód1',\n 'pieniądze ile nowy dowód2',\n 'ile koszt nowy dowód1',\n 'ile koszt nowy dowód2',\n ']',\n '- Ile płacę za wyrobienie dowodu?',\n '[',\n 'ile płacić dowód1',\n 'ile płacić dowód2',\n ']',\n '- Nie jestem zameldowany w Rzeszowie, czy mogę tutaj złożyć wniosek na dowód osobisty.',\n '[',\n 'Ja Rzeszów zameldowanie niemieć wniosek dowód1 dać można',\n 'Ja Rzeszów zameldowanie niemieć wniosek dowód2 dać można',\n 'Ja Rzeszów zameldowanie niemieć wniosek dowód1 złożyć można',\n 'Ja Rzeszów zameldowanie niemieć wniosek dowód2 złożyć można',\n ']',\n '- Proszę powtórzyć.',\n '[',\n 'powtarzać prosić',\n 'prosić powtarzać',\n ']',\n '- Chcę złożyć wniosek o wydanie dowodu osobistego.',\n '[',\n 'tu zmiana dowód1',\n 'tu zmiana dowód2',\n ']',\n '- Poproszę o wniosek.',\n '[',\n 'prosić wniosek',\n 'prosić wniosek dowód1',\n 'prosić wniosek dowód2',\n 'dowód1 wniosek prosić',\n 'dowód2 wniosek prosić',\n ']',\n '- Chcę złożyć wniosek o wydanie dowodu osobistego.',\n '[',\n 'papier wniosek mieć',\n ']',\n '- Chcę złożyć wniosek o wydanie dowodu osobistego.',\n '[',\n 'przyjść papier wniosek mieć',\n ']',\n '- Czy moje zdjęcie jest dobre?',\n '[',\n 'zdjęcie dobre',\n ']',\n '- Ile zdjęć potrzeba?',\n '[',\n 'ile sztuk zdjęcie potrzeba',\n ']',\n '- Jakie zdjęcie jest potrzebne?',\n '[',\n 'jakie zdjęcie podać potrzeba',\n 'jakie zdjęcie pokazać potrzeba',\n ']',\n '- Jak długo trzeba czekać?',\n '[',\n 'jak długo czekać',\n 'ile dni czekać',\n 'kiedy gotowy dowód1',\n 'kiedy gotowy dowód2',\n ']',\n '- Czy muszę sam odebrać dowód?',\n '[',\n 'musieć ja odebrać dowód1',\n 'musieć ja odebrać dowód2',\n ']',\n '- Zgubiłem dowód.',\n '[',\n 'dowód1 zgubić',\n 'dowód2 zgubić',\n 'zgubić dowód1',\n 'zgubić dowód2',\n ']',\n '- Mam stary dowód.',\n '[',\n 'stary dowód1 mieć',\n 'stary dowód2 mieć',\n ']',\n '- Zmieniłem(am) nazwisko.',\n '[',\n 'nazwisko zmiana1',\n 'nazwisko zmiana2',\n 'przeniesc inne spać',\n 'już ślub',\n ']',\n '- Mam 18 lat.',\n '[',\n 'już 18 lat',\n ']',\n '- Mam paszport.',\n '[',\n 'paszport mieć',\n ']']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = load_data('dataset/data.odt')\n",
    "while '' in raw_data:\n",
    "    raw_data.remove('')\n",
    "raw_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:07:17.265460157Z",
     "start_time": "2023-10-30T20:07:17.226318579Z"
    }
   },
   "id": "4d17a438678e549"
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Load model and tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8e135c67c6e659"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at ./model/model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/tokenizer\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"./model/model.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:07:23.458891713Z",
     "start_time": "2023-10-30T20:07:19.167745213Z"
    }
   },
   "id": "c1dc505bbfc6128f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Translation before training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c054916b44f525e7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'translation_text': \"Hi, I'm Kacper.\"}]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline('translation', model=model, tokenizer=tokenizer)\n",
    "translator('Czesc jestem Kacper')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:07:57.707711647Z",
     "start_time": "2023-10-30T20:07:53.504891227Z"
    }
   },
   "id": "ff063fa408d15c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adjust data form to our model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96f85ddbd2132bd3"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def split_data_from_list(raw_data):\n",
    "    pl_sentence = []\n",
    "    sentence = []\n",
    "    i=0\n",
    "    while i < len(raw_data):\n",
    "        if raw_data[i+1] == '[':\n",
    "            value = raw_data[i]\n",
    "            i += 2\n",
    "            while i < len(raw_data):\n",
    "                if raw_data[i] == ']':\n",
    "                    break\n",
    "                pl_sentence.append(value[1:])\n",
    "                sentence.append(raw_data[i])\n",
    "                i += 1\n",
    "        i += 1\n",
    "    return pl_sentence, sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:58.519228034Z",
     "start_time": "2023-10-30T20:08:58.504238232Z"
    }
   },
   "id": "b7362121004c1d7f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "s1, s2 = split_data_from_list(raw_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:59.133173693Z",
     "start_time": "2023-10-30T20:08:59.116092671Z"
    }
   },
   "id": "405202227019684b"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  pl                     mig\n",
      "0                                       Dzień dobry.             dzień dobry\n",
      "1                                       Dzień dobry.                   witać\n",
      "2   Chcę złożyć wniosek o wydanie dowodu osobistego.  ja wniosek dowód1 mieć\n",
      "3   Chcę złożyć wniosek o wydanie dowodu osobistego.  ja wniosek dowód2 mieć\n",
      "4                            Czy mogę odebrać dowód?     czy już nowy dowód1\n",
      "5                            Czy mogę odebrać dowód?     czy już nowy dowód2\n",
      "6                            Czy mogę odebrać dowód?     być mój nowy dowód1\n",
      "7                            Czy mogę odebrać dowód?     być mój nowy dowód2\n",
      "8             Chcę zgłosić utratę dowodu osobistego.        ja dowód1 zgubić\n",
      "9             Chcę zgłosić utratę dowodu osobistego.        ja dowód2 zgubić\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'pl':s1, 'mig':s2})\n",
    "print(data[0:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:59.749683883Z",
     "start_time": "2023-10-30T20:08:59.705047145Z"
    }
   },
   "id": "cb405ef16a871d7e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "430617ae1a0d0cb0"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['translation'],\n    num_rows: 123\n})"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset_list = []\n",
    "for i in range(0, len(data)):\n",
    "    raw_dataset_list.append({'translation' : {'pl' : data['pl'][i], 'mig' : data['mig'][i]}})\n",
    "    \n",
    "raw_dataset = Dataset.from_list(raw_dataset_list)\n",
    "raw_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:01.382071250Z",
     "start_time": "2023-10-30T20:09:01.364599317Z"
    }
   },
   "id": "ab17742cb745d864"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split data into train and test dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c2762491b9cd97a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_test = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_test_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test['test']})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:02.629348509Z",
     "start_time": "2023-10-30T20:09:02.613650017Z"
    }
   },
   "id": "5726fd5d5384354"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create preprocessing function for our data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e31f533b975ff10e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "max_input_length = 32\n",
    "max_target_length = 32\n",
    "source_lang = \"pl\"\n",
    "target_lang = \"mig\"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets)\n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:04.217494160Z",
     "start_time": "2023-10-30T20:09:04.187280788Z"
    }
   },
   "id": "af1985d33eb3aac5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Map preprocess function on our dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7aadcacc30862e00"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = train_test_dataset.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:06.336927442Z",
     "start_time": "2023-10-30T20:09:05.251408466Z"
    }
   },
   "id": "54bcc56f187af03b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "test_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:06.724730007Z",
     "start_time": "2023-10-30T20:09:06.338492758Z"
    }
   },
   "id": "5830af51b506f96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model preparation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a217049eebb7099b"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_marian_mt_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (TFMarianMainLayer)   multiple                  77138944  \n",
      "                                                                 \n",
      " final_logits_bias (BiasLaye  multiple                 63430     \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77,202,374\n",
      "Trainable params: 77,138,944\n",
      "Non-trainable params: 63,430\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:09.473163146Z",
     "start_time": "2023-10-30T20:09:09.394656401Z"
    }
   },
   "id": "6829cfa570b0b8c7"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'encoder',\n 'trainable': False,\n 'dtype': 'float32',\n 'config': {'vocab_size': 63430,\n  'decoder_vocab_size': 63430,\n  'max_position_embeddings': 512,\n  'd_model': 512,\n  'encoder_ffn_dim': 2048,\n  'encoder_layers': 6,\n  'encoder_attention_heads': 8,\n  'decoder_ffn_dim': 2048,\n  'decoder_layers': 6,\n  'decoder_attention_heads': 8,\n  'dropout': 0.1,\n  'attention_dropout': 0.0,\n  'activation_dropout': 0.0,\n  'activation_function': 'swish',\n  'init_std': 0.02,\n  'encoder_layerdrop': 0.0,\n  'decoder_layerdrop': 0.0,\n  'use_cache': True,\n  'num_hidden_layers': 6,\n  'scale_embedding': True,\n  'share_encoder_decoder_embeddings': True,\n  'return_dict': True,\n  'output_hidden_states': False,\n  'output_attentions': False,\n  'torchscript': False,\n  'torch_dtype': None,\n  'use_bfloat16': False,\n  'tf_legacy_loss': False,\n  'pruned_heads': {},\n  'tie_word_embeddings': True,\n  'is_encoder_decoder': True,\n  'is_decoder': False,\n  'cross_attention_hidden_size': None,\n  'add_cross_attention': False,\n  'tie_encoder_decoder': False,\n  'max_length': 512,\n  'min_length': 0,\n  'do_sample': False,\n  'early_stopping': False,\n  'num_beams': 6,\n  'num_beam_groups': 1,\n  'diversity_penalty': 0.0,\n  'temperature': 1.0,\n  'top_k': 50,\n  'top_p': 1.0,\n  'typical_p': 1.0,\n  'repetition_penalty': 1.0,\n  'length_penalty': 1.0,\n  'no_repeat_ngram_size': 0,\n  'encoder_no_repeat_ngram_size': 0,\n  'bad_words_ids': [[63429]],\n  'num_return_sequences': 1,\n  'chunk_size_feed_forward': 0,\n  'output_scores': False,\n  'return_dict_in_generate': False,\n  'forced_bos_token_id': None,\n  'forced_eos_token_id': 0,\n  'remove_invalid_values': False,\n  'exponential_decay_length_penalty': None,\n  'suppress_tokens': None,\n  'begin_suppress_tokens': None,\n  'architectures': ['MarianMTModel'],\n  'finetuning_task': None,\n  'id2label': {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'},\n  'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2},\n  'tokenizer_class': None,\n  'prefix': None,\n  'bos_token_id': 0,\n  'pad_token_id': 63429,\n  'eos_token_id': 0,\n  'sep_token_id': None,\n  'decoder_start_token_id': 63429,\n  'task_specific_params': None,\n  'problem_type': None,\n  '_name_or_path': './model/model.h5',\n  'transformers_version': '4.27.4',\n  '_num_labels': 3,\n  'add_bias_logits': False,\n  'add_final_layer_norm': False,\n  'classif_dropout': 0.0,\n  'classifier_dropout': 0.0,\n  'model_type': 'marian',\n  'normalize_before': False,\n  'normalize_embedding': False,\n  'static_position_embeddings': True}}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encoder.trainable = False\n",
    "model.model.encoder.get_config()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:26.526984047Z",
     "start_time": "2023-10-30T20:09:26.473186578Z"
    }
   },
   "id": "173df6be7f15399a"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'decoder',\n 'trainable': True,\n 'dtype': 'float32',\n 'config': {'vocab_size': 63430,\n  'decoder_vocab_size': 63430,\n  'max_position_embeddings': 512,\n  'd_model': 512,\n  'encoder_ffn_dim': 2048,\n  'encoder_layers': 6,\n  'encoder_attention_heads': 8,\n  'decoder_ffn_dim': 2048,\n  'decoder_layers': 6,\n  'decoder_attention_heads': 8,\n  'dropout': 0.1,\n  'attention_dropout': 0.0,\n  'activation_dropout': 0.0,\n  'activation_function': 'swish',\n  'init_std': 0.02,\n  'encoder_layerdrop': 0.0,\n  'decoder_layerdrop': 0.0,\n  'use_cache': True,\n  'num_hidden_layers': 6,\n  'scale_embedding': True,\n  'share_encoder_decoder_embeddings': True,\n  'return_dict': True,\n  'output_hidden_states': False,\n  'output_attentions': False,\n  'torchscript': False,\n  'torch_dtype': None,\n  'use_bfloat16': False,\n  'tf_legacy_loss': False,\n  'pruned_heads': {},\n  'tie_word_embeddings': True,\n  'is_encoder_decoder': True,\n  'is_decoder': False,\n  'cross_attention_hidden_size': None,\n  'add_cross_attention': False,\n  'tie_encoder_decoder': False,\n  'max_length': 512,\n  'min_length': 0,\n  'do_sample': False,\n  'early_stopping': False,\n  'num_beams': 6,\n  'num_beam_groups': 1,\n  'diversity_penalty': 0.0,\n  'temperature': 1.0,\n  'top_k': 50,\n  'top_p': 1.0,\n  'typical_p': 1.0,\n  'repetition_penalty': 1.0,\n  'length_penalty': 1.0,\n  'no_repeat_ngram_size': 0,\n  'encoder_no_repeat_ngram_size': 0,\n  'bad_words_ids': [[63429]],\n  'num_return_sequences': 1,\n  'chunk_size_feed_forward': 0,\n  'output_scores': False,\n  'return_dict_in_generate': False,\n  'forced_bos_token_id': None,\n  'forced_eos_token_id': 0,\n  'remove_invalid_values': False,\n  'exponential_decay_length_penalty': None,\n  'suppress_tokens': None,\n  'begin_suppress_tokens': None,\n  'architectures': ['MarianMTModel'],\n  'finetuning_task': None,\n  'id2label': {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'},\n  'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2},\n  'tokenizer_class': None,\n  'prefix': None,\n  'bos_token_id': 0,\n  'pad_token_id': 63429,\n  'eos_token_id': 0,\n  'sep_token_id': None,\n  'decoder_start_token_id': 63429,\n  'task_specific_params': None,\n  'problem_type': None,\n  '_name_or_path': './model/model.h5',\n  'transformers_version': '4.27.4',\n  '_num_labels': 3,\n  'add_bias_logits': False,\n  'add_final_layer_norm': False,\n  'classif_dropout': 0.0,\n  'classifier_dropout': 0.0,\n  'model_type': 'marian',\n  'normalize_before': False,\n  'normalize_embedding': False,\n  'static_position_embeddings': True}}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.decoder.get_config()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:27.111470894Z",
     "start_time": "2023-10-30T20:09:27.097311726Z"
    }
   },
   "id": "b1a2535857070e09"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_marian_mt_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (TFMarianMainLayer)   multiple                  77138944  \n",
      "                                                                 \n",
      " final_logits_bias (BiasLaye  multiple                 63430     \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77,202,374\n",
      "Trainable params: 25,486,336\n",
      "Non-trainable params: 51,716,038\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:27.880835180Z",
     "start_time": "2023-10-30T20:09:27.813673854Z"
    }
   },
   "id": "3b0b3269120f6c69"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.00005)\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:29.520161538Z",
     "start_time": "2023-10-30T20:09:29.445893239Z"
    }
   },
   "id": "b6db2b883e135084"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize neptune"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4023074d4d0c994"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/kacperurban/pl-mig-translation/e/PLMIG-18\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"kacperurban/pl-mig-translation\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3MTRhNjcwNy1iMzc2LTQwNTUtOGRjYy03ODI4OGQzNjkxNTEifQ==\",\n",
    "    tags='test',\n",
    ")\n",
    "\n",
    "neptune_callback = NeptuneCallback(run=run)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:10:02.712015377Z",
     "start_time": "2023-10-30T20:10:02.515868406Z"
    }
   },
   "id": "305f7ef50e6b3724"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cecfdc057fb1f1e"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 42s 813ms/step - loss: 4.4591 - accuracy: 0.1667\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 9s 790ms/step - loss: 2.4727 - accuracy: 0.3227\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 10s 812ms/step - loss: 1.7349 - accuracy: 0.3817\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 10s 786ms/step - loss: 1.2711 - accuracy: 0.4287\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 9s 792ms/step - loss: 0.9707 - accuracy: 0.4676\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 9s 800ms/step - loss: 0.7619 - accuracy: 0.5058\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 10s 811ms/step - loss: 0.5690 - accuracy: 0.4931\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 10s 815ms/step - loss: 0.4569 - accuracy: 0.5119\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 9s 766ms/step - loss: 0.3879 - accuracy: 0.5578\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 10s 828ms/step - loss: 0.3201 - accuracy: 0.5224\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 10s 790ms/step - loss: 0.2883 - accuracy: 0.5463\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 10s 780ms/step - loss: 0.2682 - accuracy: 0.5837\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 10s 861ms/step - loss: 0.2608 - accuracy: 0.5234\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 9s 781ms/step - loss: 0.2402 - accuracy: 0.6000\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 10s 844ms/step - loss: 0.2197 - accuracy: 0.5496\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 10s 829ms/step - loss: 0.2191 - accuracy: 0.5477\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 10s 816ms/step - loss: 0.2043 - accuracy: 0.5793\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 10s 848ms/step - loss: 0.1941 - accuracy: 0.5467\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 10s 795ms/step - loss: 0.1986 - accuracy: 0.5648\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 10s 882ms/step - loss: 0.1935 - accuracy: 0.5684\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 10s 854ms/step - loss: 0.1942 - accuracy: 0.5372\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 10s 824ms/step - loss: 0.1778 - accuracy: 0.5803\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 10s 825ms/step - loss: 0.1699 - accuracy: 0.5328\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 10s 830ms/step - loss: 0.1925 - accuracy: 0.5648\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 10s 831ms/step - loss: 0.1748 - accuracy: 0.5783\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 10s 839ms/step - loss: 0.1779 - accuracy: 0.5461\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 10s 825ms/step - loss: 0.1828 - accuracy: 0.5393\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 10s 870ms/step - loss: 0.1537 - accuracy: 0.5463\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 0.1643 - accuracy: 0.5606\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 10s 817ms/step - loss: 0.1504 - accuracy: 0.5824\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 10s 811ms/step - loss: 0.1554 - accuracy: 0.5614\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 10s 829ms/step - loss: 0.1492 - accuracy: 0.5484\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 10s 843ms/step - loss: 0.1561 - accuracy: 0.5648\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 10s 851ms/step - loss: 0.1442 - accuracy: 0.5489\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 10s 876ms/step - loss: 0.1444 - accuracy: 0.5682\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 10s 811ms/step - loss: 0.1490 - accuracy: 0.5952\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 0.1605 - accuracy: 0.5780\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 10s 879ms/step - loss: 0.1451 - accuracy: 0.5436\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 10s 828ms/step - loss: 0.1493 - accuracy: 0.5647\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fb674848a50>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(train_dataset, epochs=100, callbacks=[neptune_callback, early_stopping_callback])\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:17:38.782601940Z",
     "start_time": "2023-10-30T20:10:38.601060732Z"
    }
   },
   "id": "890aff221066d41d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Translation after training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfca713fbf7dbaff"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline('translation', model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:20:04.263563282Z",
     "start_time": "2023-10-30T20:20:04.255809094Z"
    }
   },
   "id": "af67263099721c3b"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'translation_text': 'witać'}]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('Dzien dobry.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:20:09.345480370Z",
     "start_time": "2023-10-30T20:20:04.823159897Z"
    }
   },
   "id": "54dc12f5d6479898"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'translation_text': 'chcieć nowy dowód2'}]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('Chcę złożyć wniosek o wydanie dowodu osobistego.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:20:12.592313738Z",
     "start_time": "2023-10-30T20:20:09.349397576Z"
    }
   },
   "id": "db743af1680e5219"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'translation_text': 'być mój nowy dowód2'}]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('Czy mogę odebrać dowód?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:20:16.463340358Z",
     "start_time": "2023-10-30T20:20:12.584405547Z"
    }
   },
   "id": "18a29736c1fc20ac"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'translation_text': 'dowód1 koniec ważny'}]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('Upłynął termin ważności dowodu.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:20:21.782121232Z",
     "start_time": "2023-10-30T20:20:16.461356128Z"
    }
   },
   "id": "4edb2bb7c593f474"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be2fe9830d4a4d8"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_corpus = []\n",
    "referance_corpus = []\n",
    "for i in range(0, len(train_test_dataset[\"test\"][\"translation\"])):\n",
    "    referance_corpus.append(train_test_dataset[\"test\"][\"translation\"][i][\"mig\"].split())\n",
    "    translation = translator(train_test_dataset[\"test\"][\"translation\"][i][\"pl\"])\n",
    "    translation_corpus.append(translation[0][\"translation_text\"].split())\n",
    "bleu_score = corpus_bleu(referance_corpus, translation_corpus)\n",
    "bleu_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:47:01.119514876Z",
     "start_time": "2023-10-30T20:45:02.581472127Z"
    }
   },
   "id": "91795087a2d9d73f"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "                            referance                         translation\n0    [ja, nie, rozumieć, co, to, być]                  [jak, pisać, mieć]\n1         [ja, wniosek, dowód1, mieć]              [chcieć, nowy, dowód2]\n2          [ile, koszt, nowy, dowód1]      [pieniądze, ile, nowy, dowód1]\n3                 [mój, dowód1, psuć]                [mój, dowód2, pękąć]\n4      [pieniądze, ile, nowy, dowód2]      [pieniądze, ile, nowy, dowód1]\n5            [czy, już, nowy, dowód1]            [być, mój, nowy, dowód2]\n6         [ja, wniosek, dowód2, mieć]              [chcieć, nowy, dowód2]\n7          [ile, koszt, nowy, dowód2]      [pieniądze, ile, nowy, dowód1]\n8  [ja, pierwszy, raz, dowód1, robić]  [ja, pierwszy, raz, dowód2, robić]\n9                      [dzień, dobry]                             [witać]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>referance</th>\n      <th>translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[ja, nie, rozumieć, co, to, być]</td>\n      <td>[jak, pisać, mieć]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[ja, wniosek, dowód1, mieć]</td>\n      <td>[chcieć, nowy, dowód2]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[ile, koszt, nowy, dowód1]</td>\n      <td>[pieniądze, ile, nowy, dowód1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[mój, dowód1, psuć]</td>\n      <td>[mój, dowód2, pękąć]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[pieniądze, ile, nowy, dowód2]</td>\n      <td>[pieniądze, ile, nowy, dowód1]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[czy, już, nowy, dowód1]</td>\n      <td>[być, mój, nowy, dowód2]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[ja, wniosek, dowód2, mieć]</td>\n      <td>[chcieć, nowy, dowód2]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[ile, koszt, nowy, dowód2]</td>\n      <td>[pieniądze, ile, nowy, dowód1]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[ja, pierwszy, raz, dowód1, robić]</td>\n      <td>[ja, pierwszy, raz, dowód2, robić]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[dzień, dobry]</td>\n      <td>[witać]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_ref_trans = pd.DataFrame({\"referance\" : referance_corpus, \"translation\" : translation_corpus})\n",
    "compare_ref_trans.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:50:34.816548290Z",
     "start_time": "2023-10-30T20:50:34.715653007Z"
    }
   },
   "id": "bb07d37e62cf1675"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
